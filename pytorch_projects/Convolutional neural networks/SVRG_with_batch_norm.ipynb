{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Implementing SVRG in a state-of-the-art CNN model</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Pytorch and other useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.gray()\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "\n",
    "def accuracy(Y_hat, Y):\n",
    "    preds = torch.argmax(Y_hat, dim=1)\n",
    "    return (preds == Y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples :  60000\n",
      "Test examples :  10000\n",
      "Nb of features :  1\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=None)\n",
    "\n",
    "#load trainset into tensors\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=1, shuffle=True)\n",
    "X_train = train_loader.dataset.data\n",
    "Y_train = train_loader.dataset.targets\n",
    "\n",
    "#load testset into tensors\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10000, shuffle=False)\n",
    "X_test = test_loader.dataset.data\n",
    "Y_test = test_loader.dataset.targets\n",
    "\n",
    "#scale data to [0:1] and convert to float32\n",
    "X_train = (X_train.to(dtype=torch.float32) / X_train.max().to(dtype=torch.float32))\n",
    "X_test = (X_test.to(dtype=torch.float32) / X_test.max().to(dtype=torch.float32))\n",
    "\n",
    "#Flatten train and test data\n",
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)\n",
    "\n",
    "print(\"Train examples : \",X_train.shape[0])\n",
    "print(\"Test examples : \",X_test.shape[0])\n",
    "print(\"Nb of features : \",X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(784,100)\n",
    "        self.linear2 = nn.Linear(100,10)\n",
    "        self.linear1_snapshot = nn.Linear(784,100)\n",
    "        self.linear2_snapshot = nn.Linear(100,10)\n",
    "        \n",
    "        self.number_params = 4 \n",
    "        \n",
    "        self.mu = [None] * self.number_params\n",
    "        \n",
    "        self.copy_snapshot()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.softmax(self.linear2(x),1)\n",
    "        return x\n",
    "    \n",
    "    def forward_snapshot(self, x):\n",
    "        x = torch.sigmoid(self.linear1_snapshot(x))\n",
    "        x = torch.softmax(self.linear2_snapshot(x),1)\n",
    "        return x\n",
    "    \n",
    "    def copy_snapshot(self):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params):\n",
    "            params[i+self.number_params].data.copy_(params[i])\n",
    "\n",
    "        i=0\n",
    "        for param in self.parameters():\n",
    "            if (i < self.number_params) :\n",
    "                self.mu[i] = torch.zeros(param.shape)\n",
    "                i+=1\n",
    "\n",
    "    def update_SGD(self, lr=1):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params):\n",
    "            params[i].data.copy_(params[i].data - lr * params[i].grad.data)\n",
    "\n",
    "    def update_SVRG(self,lr):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params):\n",
    "            params[i].data.copy_(params[i].data - lr * (params[i].grad.data - params[i+self.number_params].grad.data + self.mu[i].data))       \n",
    "\n",
    "    def update_mu(self,n,batch_size):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(len(self.mu)):\n",
    "            self.mu[i].data.copy_(self.mu[i].data + params[i+self.number_params].grad.data / (n/batch_size))\n",
    "                            \n",
    "    def fit_SVRG(self,optimizer,epochs,warm_epochs,n,batch_size,lr):\n",
    "        params = list(self.parameters())\n",
    "        \n",
    "        n = X_train.shape[0]\n",
    "        self.train()\n",
    "        \n",
    "        #Warm start\n",
    "        for epoch in range(warm_epochs):\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward( X )\n",
    "                loss = loss_func( pred , Y )\n",
    "                loss.backward()\n",
    "                self.update_SGD(0.1)\n",
    "                \n",
    "            print(epoch,\"\\t\",loss.item())\n",
    "\n",
    "        self.copy_snapshot()\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            #update mu\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward_snapshot( X )\n",
    "                loss_snapshot = loss_func( pred , Y )\n",
    "\n",
    "                loss_snapshot.backward()\n",
    "\n",
    "                self.update_mu(n,batch_size)\n",
    "            \n",
    "            \n",
    "            for m in range(5):\n",
    "                for i in range((n - 1) // batch_size + 1):\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    #Snapshot gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    X = self.forward_snapshot( X )\n",
    "                    loss_snapshot = loss_func( X , Y )\n",
    "                    loss_snapshot.backward()\n",
    "                    #'real' gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    X = self.forward( X )\n",
    "                    loss = loss_func( X , Y )\n",
    "                    loss.backward()\n",
    "                    self.update_SVRG(lr)\n",
    "                    \n",
    "                print(epoch * 2 + m+warm_epochs,\"\\t\",loss.item())\n",
    "            \n",
    "            self.copy_snapshot()\n",
    "            with torch.no_grad():\n",
    "                self.eval()\n",
    "                print(\"Test set \\t\", round(accuracy( self.forward(X_test.reshape(-1,784)) , Y_test).item(),3))\n",
    "\n",
    "    def fit_SGD(self,optimizer,epochs,batch_size,lr):\n",
    "        n = X_train.shape[0]\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward( X )\n",
    "                loss = loss_func( pred , Y )\n",
    "                loss.backward()\n",
    "                self.update_SGD(lr)\n",
    "                \n",
    "            print(epoch,\"\\t\",loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.eval()\n",
    "                print(\"Test set \\t\", round(accuracy( self.forward(X_test.reshape(-1,784)) , Y_test).item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 2.152837038040161\n",
      "1 \t 1.9950679540634155\n",
      "2 \t 1.9186946153640747\n",
      "3 \t 1.84818696975708\n",
      "4 \t 1.7446726560592651\n",
      "5 \t 1.6252782344818115\n",
      "6 \t 1.5430420637130737\n",
      "7 \t 1.5173792839050293\n",
      "8 \t 1.499821424484253\n",
      "9 \t 1.4892243146896362\n",
      "Test set \t 0.931\n",
      "7 \t 1.4814927577972412\n",
      "8 \t 1.4791216850280762\n",
      "9 \t 1.4777387380599976\n",
      "10 \t 1.4771500825881958\n",
      "11 \t 1.4767204523086548\n",
      "Test set \t 0.945\n",
      "9 \t 1.4711124897003174\n",
      "10 \t 1.470357894897461\n",
      "11 \t 1.4697270393371582\n",
      "12 \t 1.4691221714019775\n",
      "13 \t 1.4685349464416504\n",
      "Test set \t 0.952\n",
      "11 \t 1.4695355892181396\n",
      "12 \t 1.4691652059555054\n",
      "13 \t 1.4688200950622559\n",
      "14 \t 1.4684512615203857\n",
      "15 \t 1.4680485725402832\n",
      "Test set \t 0.956\n",
      "13 \t 1.4669963121414185\n",
      "14 \t 1.4667611122131348\n",
      "15 \t 1.4665948152542114\n",
      "16 \t 1.4664931297302246\n",
      "17 \t 1.466439127922058\n",
      "Test set \t 0.962\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000,784)\n",
    "\n",
    "simple_mod = SimpleNet()\n",
    "opt = optim.SGD(simple_mod.parameters(), lr=10)\n",
    "epochs = 5\n",
    "warm_epochs = 5\n",
    "batch_size = 60\n",
    "learning_rate = 0.5\n",
    "\n",
    "#simple_mod.fit_SGD(opt,epochs,batch_size,learning_rate)\n",
    "simple_mod.fit_SVRG(opt,epochs,warm_epochs,X_train.shape[0],batch_size,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=5, stride=1, padding=2)\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 48, kernel_size=5, stride=1, padding=2)\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(48)\n",
    "        self.conv3 = nn.Conv2d(48, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.max3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.linear4 = nn.Linear(64*3*3,256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.linear5 = nn.Linear(256,10)\n",
    "        \n",
    "        self.number_params = 18\n",
    "        \n",
    "        self.mu = [None] * self.number_params\n",
    "        \n",
    "        self.copy_snapshot()\n",
    " \n",
    "    def forward(self, x):\n",
    "        #print(\"--------FORWARD---------\")\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        #print(\"conv1 :\" , x.shape)\n",
    "        x = self.max1(x)\n",
    "        x = self.bn1(x)\n",
    "        #print(\"max1 :\" , x.shape)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        #print(\"conv2 :\" , x.shape)\n",
    "        x = self.max2(x)\n",
    "        x = self.bn2(x)\n",
    "        #print(\"max2 :\" , x.shape)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        #print(\"conv3 :\" , x.shape)\n",
    "        x = self.max3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"max3 :\" , x.shape)\n",
    "        x = self.linear4(torch.relu(x.reshape(x.shape[0],-1)))\n",
    "        #print(\"linear4 :\" , x.shape)\n",
    "        x = self.bn4(x)\n",
    "        x = self.linear5(torch.softmax(x,1))\n",
    "        #print(\"linear5 :\" , x.shape)\n",
    "        return x\n",
    "    \n",
    "    def forward_snapshot(self, x):\n",
    "        #print(\"--------FORWARD---------\")\n",
    "        x = torch.relu(self.conv1_snapshot(x))\n",
    "        #print(\"conv1 :\" , x.shape)\n",
    "        x = self.max1(x)\n",
    "        x = self.bn1_snapshot(x)\n",
    "        #print(\"max1 :\" , x.shape)\n",
    "        x = torch.relu(self.conv2_snapshot(x))\n",
    "        #print(\"conv2 :\" , x.shape)\n",
    "        x = self.max2(x)\n",
    "        x = self.bn2_snapshot(x)\n",
    "        #print(\"max2 :\" , x.shape)\n",
    "        x = torch.relu(self.conv3_snapshot(x))\n",
    "        #print(\"conv3 :\" , x.shape)\n",
    "        x = self.max3(x)\n",
    "        x = self.bn3_snapshot(x)\n",
    "        #print(\"max3 :\" , x.shape)\n",
    "        x = self.linear4_snapshot(torch.relu(x.reshape(x.shape[0],-1)))\n",
    "        #print(\"linear4 :\" , x.shape)\n",
    "        x = self.bn4_snapshot(x)\n",
    "        x = self.linear5_snapshot(torch.softmax(x,1))\n",
    "        #print(\"linear5 :\" , x.shape)\n",
    "        return x\n",
    "    \n",
    "    def copy_snapshot(self):\n",
    "        self.conv1_snapshot = copy.deepcopy(self.conv1)\n",
    "        self.bn1_snapshot = copy.deepcopy(self.bn1)\n",
    "        self.conv2_snapshot = copy.deepcopy(self.conv2)\n",
    "        self.bn2_snapshot = copy.deepcopy(self.bn2)\n",
    "        self.conv3_snapshot = copy.deepcopy(self.conv3)\n",
    "        self.bn3_snapshot = copy.deepcopy(self.bn3)\n",
    "        self.linear4_snapshot = copy.deepcopy(self.linear4)\n",
    "        self.bn4_snapshot = copy.deepcopy(self.bn4)\n",
    "        self.linear5_snapshot = copy.deepcopy(self.linear5)\n",
    "\n",
    "        i=0\n",
    "        for param in self.parameters():\n",
    "            if (i < self.number_params) :\n",
    "                self.mu[i] = torch.zeros(param.shape)\n",
    "                i+=1\n",
    "\n",
    "    def update_SGD(self, lr=1):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params):\n",
    "            params[i].data.copy_(params[i].data - lr * params[i].grad.data)\n",
    "\n",
    "    def update_SVRG(self,lr):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params):\n",
    "            params[i].data.copy_(params[i].data - lr * (params[i].grad.data - params[i+self.number_params].grad.data + self.mu[i].data))       \n",
    "\n",
    "    def update_mu(self,batch_size):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(len(self.mu)):\n",
    "            self.mu[i].data.copy_(self.mu[i].data + params[i+self.number_params].grad.data / 5)\n",
    "                            \n",
    "    def fit_SVRG(self,optimizer,epochs,warm_epochs,batch_size,lr):\n",
    "        n = X_train.shape[0]\n",
    "        model.train()\n",
    "        \n",
    "        #Warm start\n",
    "        for epoch in range(warm_epochs):\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward( X )\n",
    "                loss = loss_func( pred , Y )\n",
    "                loss.backward()\n",
    "                self.update_SGD(10)\n",
    "                \n",
    "            print(epoch,\"\\t\",loss.item())\n",
    "\n",
    "        self.copy_snapshot()\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            #update mu\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward_snapshot( X )\n",
    "                loss_snapshot = loss_func( pred , Y )\n",
    "                loss_snapshot.backward()\n",
    "                self.update_mu(batch_size)\n",
    "            \n",
    "            \n",
    "            for m in range(5):\n",
    "                for i in range((n - 1) // batch_size + 1):\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    #Snapshot gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    pred = self.forward_snapshot( X )\n",
    "                    loss_snapshot = loss_func( pred , Y )\n",
    "                    loss_snapshot.backward()\n",
    "                    \n",
    "                    #'real' gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    pred = self.forward( X )\n",
    "                    loss = loss_func( pred , Y )\n",
    "                    loss.backward()\n",
    "                    self.update_SVRG(lr)\n",
    "                    \n",
    "           \n",
    "                print(epoch * 2 + m+warm_epochs,\"\\t\",loss.item())\n",
    "            \n",
    "            self.copy_snapshot()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                print(\"Test set \\t\", round(accuracy( model.forward(X_test) , Y_test).item(),3))\n",
    "\n",
    "    def fit_SGD(self,optimizer,epochs,batch_size,lr):\n",
    "        n = X_train.shape[0]\n",
    "       \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward( X )\n",
    "                loss = loss_func( pred , Y )\n",
    "                loss.backward()\n",
    "                self.update_SGD(lr)\n",
    "                \n",
    "            print(epoch,\"\\t\",loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                print(\"Test set \\t\", round(accuracy( model.forward(X_test) , Y_test).item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 2.327699661254883\n",
      "1 \t 2.311671495437622\n",
      "2 \t 2.2557857036590576\n",
      "3 \t 1.9566709995269775\n",
      "4 \t 1.9679818153381348\n",
      "5 \t 1.7659927606582642\n",
      "6 \t 1.854134202003479\n",
      "7 \t 1.9519540071487427\n",
      "8 \t 2.063992977142334\n",
      "9 \t 2.209808588027954\n",
      "Test set \t 0.171\n",
      "7 \t 2.315607786178589\n",
      "8 \t 2.581749200820923\n",
      "9 \t 3.146223545074463\n",
      "10 \t 4.1037116050720215\n",
      "11 \t 5.4671831130981445\n",
      "Test set \t 0.096\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "opt = optim.SGD(model.parameters(), lr=10)\n",
    "epochs = 2\n",
    "warm_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "model.fit_SVRG(opt,epochs,warm_epochs,batch_size,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load, Preprocess and predict test set from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from CSV\n",
    "test = pd.read_csv('../data/MNIST/test.csv')\n",
    "test_tensor = torch.tensor(test.values)\n",
    "\n",
    "#Preprocess\n",
    "test_tensor = (test_tensor.to(dtype=torch.float32) / test_tensor.max().to(dtype=torch.float32))\n",
    "test_tensor = test_tensor.reshape(test_tensor.shape[0],1,28,28)\n",
    "\n",
    "#Predict\n",
    "test_tensor = model.forward(test_tensor)\n",
    "test_tensor = test_tensor.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fb62caacb33f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Convert to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# write CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/MNIST/predictions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "#Convert to a numpy array\n",
    "arr = test_tensor.numpy()\n",
    "\n",
    "# write CSV\n",
    "np.savetxt('../data/MNIST/predictions.csv', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.mu)):\n",
    "    print(model.mu[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-ce5db941cd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params[10].grad.data     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
