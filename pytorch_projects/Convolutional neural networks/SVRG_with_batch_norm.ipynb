{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>A comprehensive example of CNN with Pytorch</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.*\n",
    "\n",
    "__Deep Learning__, I. Goodfellow & al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our aim in this section is to use multiple methods introduced by CNN in order to outperform simple fully connected neural networks. We will first describe each method and explain the motivation behind. Then, we will train a model using all these methods and compare it to simple neural networks. \n",
    "\n",
    "Note : we will mostly use __Deep Learning__, I. Goodfellow & al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"table\"></a>\n",
    "- **I- [The convolution operator as a network simplifier](#convolution)**\n",
    "\t- 1- [Principle of convolution](#principle_conv)\n",
    "\t- 2- [Motivation behind convolution](#motivation_conv)\n",
    "- **II- [Pooling to improve statistical robustness](#pooling)**\n",
    "\t- 1- [What is pooling ?](#what_pool)\n",
    "\t- 2- [Different ways of pooling](#diff_pool)\n",
    "    - 3- [Pooling is useful for object detection](#detect_pool)\n",
    "- **III- [Batch normalization to reduce internal covariate shift](#batch)**\n",
    "\t- 1- [The problem of the internal covariate shift](#covariate)\n",
    "    - 2- [The method of Batch Normalization](#method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"convolution\"></a>\n",
    "# I- The convolution operator as a network simplifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III- Creating and training a CNN model on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Pytorch and other useful librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.gray()\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "\n",
    "def accuracy(Y_hat, Y):\n",
    "    preds = torch.argmax(Y_hat, dim=1)\n",
    "    return (preds == Y).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples :  60000\n",
      "Test examples :  10000\n",
      "Nb of features :  1\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=None)\n",
    "\n",
    "#load trainset into tensors\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=1, shuffle=True)\n",
    "X_train = train_loader.dataset.data\n",
    "Y_train = train_loader.dataset.targets\n",
    "\n",
    "#load testset into tensors\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10000, shuffle=False)\n",
    "X_test = test_loader.dataset.data\n",
    "Y_test = test_loader.dataset.targets\n",
    "\n",
    "#scale data to [0:1] and convert to float32\n",
    "X_train = (X_train.to(dtype=torch.float32) / X_train.max().to(dtype=torch.float32))\n",
    "X_test = (X_test.to(dtype=torch.float32) / X_test.max().to(dtype=torch.float32))\n",
    "\n",
    "#Flatten train and test data\n",
    "X_train = X_train.reshape(X_train.shape[0],1,28,28)\n",
    "X_test = X_test.reshape(X_test.shape[0],1,28,28)\n",
    "\n",
    "print(\"Train examples : \",X_train.shape[0])\n",
    "print(\"Test examples : \",X_test.shape[0])\n",
    "print(\"Nb of features : \",X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=5, stride=1, padding=2)\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 48, kernel_size=5, stride=1, padding=2)\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(48)\n",
    "        self.conv3 = nn.Conv2d(48, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.max3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.linear4 = nn.Linear(64*3*3,256)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.linear5 = nn.Linear(256,10)\n",
    "        \n",
    "        self.number_params = 18\n",
    "        \n",
    "        self.mu = [None] * self.number_params\n",
    "        \n",
    "        self.copy_snapshot()\n",
    " \n",
    "    def forward(self, x):\n",
    "        #print(\"--------FORWARD---------\")\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        #print(\"conv1 :\" , x.shape)\n",
    "        x = self.max1(x)\n",
    "        x = self.bn1(x)\n",
    "        #print(\"max1 :\" , x.shape)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        #print(\"conv2 :\" , x.shape)\n",
    "        x = self.max2(x)\n",
    "        x = self.bn2(x)\n",
    "        #print(\"max2 :\" , x.shape)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        #print(\"conv3 :\" , x.shape)\n",
    "        x = self.max3(x)\n",
    "        x = self.bn3(x)\n",
    "        #print(\"max3 :\" , x.shape)\n",
    "        x = self.linear4(torch.relu(x.reshape(x.shape[0],-1)))\n",
    "        #print(\"linear4 :\" , x.shape)\n",
    "        x = self.bn4(x)\n",
    "        x = self.linear5(torch.softmax(x,1))\n",
    "        #print(\"linear5 :\" , x.shape)\n",
    "        return x\n",
    "    \n",
    "    def forward_snapshot(self, x):\n",
    "        #print(\"--------FORWARD---------\")\n",
    "        x = torch.relu(self.conv1_snapshot(x))\n",
    "        #print(\"conv1 :\" , x.shape)\n",
    "        x = self.max1(x)\n",
    "        x = self.bn1_snapshot(x)\n",
    "        #print(\"max1 :\" , x.shape)\n",
    "        x = torch.relu(self.conv2_snapshot(x))\n",
    "        #print(\"conv2 :\" , x.shape)\n",
    "        x = self.max2(x)\n",
    "        x = self.bn2_snapshot(x)\n",
    "        #print(\"max2 :\" , x.shape)\n",
    "        x = torch.relu(self.conv3_snapshot(x))\n",
    "        #print(\"conv3 :\" , x.shape)\n",
    "        x = self.max3(x)\n",
    "        x = self.bn3_snapshot(x)\n",
    "        #print(\"max3 :\" , x.shape)\n",
    "        x = self.linear4_snapshot(torch.relu(x.reshape(x.shape[0],-1)))\n",
    "        #print(\"linear4 :\" , x.shape)\n",
    "        x = self.bn4_snapshot(x)\n",
    "        x = self.linear5_snapshot(torch.softmax(x,1))\n",
    "        #print(\"linear5 :\" , x.shape)\n",
    "        return x\n",
    "    \n",
    "    def copy_snapshot(self):\n",
    "        self.conv1_snapshot = copy.deepcopy(self.conv1)\n",
    "        self.bn1_snapshot = copy.deepcopy(self.bn1)\n",
    "        self.conv2_snapshot = copy.deepcopy(self.conv2)\n",
    "        self.bn2_snapshot = copy.deepcopy(self.bn2)\n",
    "        self.conv3_snapshot = copy.deepcopy(self.conv3)\n",
    "        self.bn3_snapshot = copy.deepcopy(self.bn3)\n",
    "        self.linear4_snapshot = copy.deepcopy(self.linear4)\n",
    "        self.bn4_snapshot = copy.deepcopy(self.bn4)\n",
    "        self.linear5_snapshot = copy.deepcopy(self.linear5)\n",
    "\n",
    "        i=0\n",
    "        for param in self.parameters():\n",
    "            if (i < self.number_params) :\n",
    "                self.mu[i] = torch.zeros(param.shape)\n",
    "                i+=1\n",
    "\n",
    "    def update_SGD(self, lr=0.1):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(self.number_params // 2,self.number_params):\n",
    "            params[i].data = params[i].data - lr * params[i].grad.data\n",
    "        \n",
    "    def update_SVRG(self,lr):\n",
    "        params = list(self.parameters())\n",
    "        k = len(params) // 2\n",
    "        for i in range(k):\n",
    "            params[i].data = params[i].data - lr * (params[i].grad.data - params[i+k].grad.data + self.mu[i].data)     \n",
    "    \n",
    "    def update_mu(self,batch_size):\n",
    "        params = list(self.parameters())\n",
    "        for i in range(len(self.mu)):\n",
    "            self.mu[i].data = self.mu[i].data + params[i+self.number_params].grad.data / batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "                     \n",
    "                            \n",
    "    def fit(self,optimizer,epochs,batch_size,lr,decay):\n",
    "        n = X_train.shape[0]\n",
    "        model.train()\n",
    "        \n",
    "        #Warm start\n",
    "        for _ in range(3):\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward( X )\n",
    "                loss = loss_func( pred , Y )\n",
    "                loss.backward()\n",
    "                self.update_SGD()\n",
    "                \n",
    "            print(\"0\\t\",loss.item())\n",
    "\n",
    "        self.copy_snapshot()\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            #update mu\n",
    "            for i in range((n - 1) // batch_size + 1):\n",
    "                optimizer.zero_grad()\n",
    "                X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                pred = self.forward_snapshot( X )\n",
    "                loss_snapshot = loss_func( pred , Y )\n",
    "                loss_snapshot.backward()\n",
    "                self.update_mu(batch_size)\n",
    "            \n",
    "            \n",
    "            for m in range(5):\n",
    "                for i in range((n - 1) // batch_size + 1):\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    #Snapshot gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    pred = self.forward_snapshot( X )\n",
    "                    loss_snapshot = loss_func( pred , Y )\n",
    "                    loss_snapshot.backward()\n",
    "                    \n",
    "                    #'real' gradient computation\n",
    "                    X = X_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    Y = Y_train[ i * batch_size : (i+1) * batch_size ]\n",
    "                    pred = self.forward( X )\n",
    "                    loss = loss_func( pred , Y )\n",
    "                    loss.backward()\n",
    "                    self.update_SVRG(lr)\n",
    "                    \n",
    "           \n",
    "                print(epoch * 5 + m+1,\"\\t\",loss.item())\n",
    "            \n",
    "            self.copy_snapshot()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                print(\"Test set \\t\", round(accuracy( model.forward(X_test) , Y_test).item(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t 2.3003013134002686\n",
      "0\t 2.298699378967285\n",
      "0\t 2.2973012924194336\n",
      "1 \t 2.296299457550049\n",
      "2 \t 2.296624183654785\n",
      "3 \t 2.2971198558807373\n",
      "4 \t 2.2977921962738037\n",
      "5 \t 2.2986438274383545\n",
      "Test set \t 0.103\n",
      "6 \t 2.298912286758423\n",
      "7 \t 2.2992868423461914\n",
      "8 \t 2.2998552322387695\n",
      "9 \t 2.3006157875061035\n",
      "10 \t 2.3015732765197754\n",
      "Test set \t 0.102\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=1)\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model.fit(opt,epochs,batch_size,learning_rate,decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for tens in model.parameters():\n",
    "    print(tens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load, Preprocess and predict test set from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from CSV\n",
    "test = pd.read_csv('../data/MNIST/test.csv')\n",
    "test_tensor = torch.tensor(test.values)\n",
    "\n",
    "#Preprocess\n",
    "test_tensor = (test_tensor.to(dtype=torch.float32) / test_tensor.max().to(dtype=torch.float32))\n",
    "test_tensor = test_tensor.reshape(test_tensor.shape[0],1,28,28)\n",
    "\n",
    "#Predict\n",
    "test_tensor = model.forward(test_tensor)\n",
    "test_tensor = test_tensor.argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fb62caacb33f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Convert to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# write CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/MNIST/predictions.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "#Convert to a numpy array\n",
    "arr = test_tensor.numpy()\n",
    "\n",
    "# write CSV\n",
    "np.savetxt('../data/MNIST/predictions.csv', arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model.mu)):\n",
    "    print(model.mu[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n",
      "torch.Size([24, 1, 5, 5])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([48, 24, 5, 5])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([64, 48, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([256, 576])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([10, 256])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
